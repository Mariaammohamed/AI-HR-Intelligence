{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKtpaMspB9vz"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0ab5cEQB9v0"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjXG_e8xB9v1"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380,
     "referenced_widgets": [
      "bab617670c354c10b6d1b6e0228523ef",
      "a9a2e5cd35244542976c6bd0ac1c0106",
      "4fb2c8739019447cb4d23ec3669f596c",
      "970e26c8a1f6407182df7f7462cd6e09",
      "a15e02fb15604dcc8db1114c92e17ab6",
      "373c4cbdd7de4a1884819ea0f3519980",
      "cfc6315fe3054ab4967394321984e81d",
      "502befa9b962445c924e8e22e6b40fd8",
      "0eaf97d92fc34a97b998a9d8338bbed6",
      "1faa6c66fc79424a93cc20e70ba2136a",
      "5ff05a45a07f458a913c9d45ee36aa12",
      "af9843b0bb7a48af8dc9b21b5a0a3a21",
      "84d88d7628b341d7bc9d2c1635fdd948",
      "2029fe11b4cb4ae182267f88a9fb8d16",
      "fb771ef7a4ad48468e5459ab23b1c954",
      "1937a3a73eb24400a426e7ec668d8fcc",
      "65a1affc43bb436ab34aa296815db3b3",
      "7be75661665e470d81fcff23022b883b",
      "7f654fde54e84f55ae76222399efffa1",
      "a2103e63aeab42c9a9063bc93828cb01",
      "bb34506099fc4737925bbe69de45f76d",
      "42ba183e00014b4395520b4d98c78170",
      "df1f816102b84e52bc0a9f744444463e",
      "50fe4a3f4f10433bb5319486e5a0091a",
      "8224d63b83b140b188ec472786a2f74e",
      "bf37a0c70883476c9ced3fef8cedc752",
      "caa7b4473351494187a41ba3df73cd24",
      "504bd5b1fb41497ebd3b9642c022138a",
      "0dcc7c3bac7f4c1d8f36058e4a84aac3",
      "dea0a718614b45969aaf8910f1deac66",
      "7edefccc5efb41819c41b445dbe7a7bc",
      "1816ee18c84a492a9587199b6eca6500",
      "46d08c76b6a64deebdf382720aa62b8d",
      "d8823b112e554c4d8d323fb1e95d9511",
      "f73be348fef844e6b02f68d356e91a54",
      "1b3bfb96f1b8480680df3e5d2eaaef88",
      "edc591785f554b668645b0e1fe836796",
      "cd8048aef911448d87c1a3a7fae7e530",
      "47e7b4ba13614695a93f4035ff86a879",
      "ddf968e9b44949eebb71f9ac670898e4",
      "628925592646451e9bf1143fede34d26",
      "e0fd8749c5b641d38e28f249e5038e0f",
      "d175442e5f254394942fcedf3cf43a62",
      "f7f7e8f8f0484fee81d10190b77fdfdc",
      "1de764d12abc45cf846cf163ed33e2e0",
      "91518ffef8294eeab08ac4e29fe15e46",
      "bb637246be614dbb89e82ac0fdee8c5e",
      "a7576836020346d0bd17216da812afda",
      "37422660f4074b55830f82e65830fad7",
      "526485085ea64b16bdeca4c4c53ca519",
      "4a06986c0c7c4008ab283f133f9fbd30",
      "aa6b94ed081a4ae8bb58b4d9e2234c9a",
      "ecca65f395234d0685cce06950993c5b",
      "b8bc2c11aa0646baa8d865e6001f28fa",
      "ae2d389b086a4e9db4ca4e44a50c1f51",
      "f418172e743b44d6b5be3d20f2b2be68",
      "2a26a9a615e2424da444c44761847de8",
      "95cb9d7e62194dfc8679515391491f54",
      "95829d0c8b50458ea22f2336686dfd24",
      "9bef35a324b141c0aa4403cfeab859c2",
      "bf105f1fb65141f1a7c78c3bf99a35df",
      "fd12f444231d44febeedb470bea18d66",
      "ce38c245f7914bdb958788d3bee011d8",
      "406cbfa34e984cb1978f94c47a72a99b",
      "4022c69b608d424eb73c8484d1649514",
      "1e507cea5c9541f9bfe9a40290121208",
      "a99a1a247eb742ed8a4997bcee98bea0",
      "1476b8b1316e4691bc96bba76d559f3e",
      "caa134702d0b4463860139935c6cba34",
      "fb1178c278564e18b385a027695a18cc",
      "c89c05329b1049f19138d3eaffcfe805",
      "4b6797fb1fca47e19cade6d185d8b171",
      "102f6988ab664871a2f2c60a51a8926a",
      "3ae16dcf8b1e4704a5ecca167b3a6dba",
      "052d99a98faa443e8e292e7dc8975518",
      "0cccc6197056499b9b1ae756a3f5482a",
      "aa03c9f3d3c04686aca6f94e7c647167"
     ]
    },
    "executionInfo": {
     "elapsed": 109684,
     "status": "ok",
     "timestamp": 1768436754063,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "43795c09-7b33-464c-931b-10c15dffa91a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.1.2: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab617670c354c10b6d1b6e0228523ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9843b0bb7a48af8dc9b21b5a0a3a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/140 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1f816102b84e52bc0a9f744444463e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8823b112e554c4d8d323fb1e95d9511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de764d12abc45cf846cf163ed33e2e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f418172e743b44d6b5be3d20f2b2be68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99a1a247eb742ed8a4997bcee98bea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Phi-3.5-mini-instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10371,
     "status": "ok",
     "timestamp": 1768436764440,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "65226e96-defa-4430-80b8-0c9a77f9586f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2026.1.2 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We now use the `Phi-3` format for conversation style finetunes. We use [Open Assistant conversations](https://huggingface.co/datasets/philschmid/guanaco-sharegpt-style) in ShareGPT style. Phi-3 renders multi turn conversations like below:\n",
    "\n",
    "```\n",
    "<|user|>\n",
    "Hi!<|end|>\n",
    "<|assistant|>\n",
    "Hello! How are you?<|end|>\n",
    "<|user|>\n",
    "I'm doing great! And you?<|end|>\n",
    "\n",
    "```\n",
    "\n",
    "**[NOTE]** To train only on completions (ignoring the user's input) read Unsloth's docs [here](https://github.com/unslothai/unsloth/wiki#train-on-completions--responses-only-do-not-train-on-inputs).\n",
    "\n",
    "We use our `get_chat_template` function to get the correct chat template. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old` and our own optimized `unsloth` template.\n",
    "\n",
    "Note ShareGPT uses `{\"from\": \"human\", \"value\" : \"Hi\"}` and not `{\"role\": \"user\", \"content\" : \"Hi\"}`, so we use `mapping` to map it.\n",
    "\n",
    "For text completions like novel writing, try this [notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjUTu2CGFGB4"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Majinuub/Resume_Parsing\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8YblQaPFKeZ"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "converted = []\n",
    "\n",
    "for ex in ds:\n",
    "    converted.append({\n",
    "        \"conversations\": [\n",
    "            {\n",
    "                \"from\": \"human\",\n",
    "                \"value\": \"Extract structured resume data from this CV:\\n\\n\" + ex[\"Input\"]\n",
    "            },\n",
    "            {\n",
    "                \"from\": \"gpt\",\n",
    "                \"value\": ex[\"Output\"]\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "\n",
    "with open(\"train.json\", \"w\") as f:\n",
    "    for row in converted:\n",
    "        f.write(json.dumps(row) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "863a37b7a7da4f24a04e168eabbd7a94",
      "567acc6e311d48c6b57f93f97942641e",
      "de98299f8cbc4abfb85a503d30c5ad75",
      "68aa454d403a404885109188fadbbe28",
      "1391b0ed1f3241959762e5b1724a4e06",
      "c38dafe4a25547a094435da1532a0e55",
      "30bc877e54734aca96ba241f4ddc91f7",
      "09861ded88a94588b7c8c1ccd2dc9dde",
      "d96319f6c76b4656b3fb178c9afae9a6",
      "d4e0135da73242fe9ecf77a1f799229d",
      "21e8cb8f0982438b96f5db4c147aef47"
     ]
    },
    "executionInfo": {
     "elapsed": 827,
     "status": "ok",
     "timestamp": 1768437773163,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "k7HUSlFZGtnF",
    "outputId": "00fefb68-85cd-4e17-f94b-fc97c5a5fc25"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863a37b7a7da4f24a04e168eabbd7a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"train.json\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OzN71Yv-HNGN"
   },
   "outputs": [],
   "source": [
    "def format_chat(example):\n",
    "    messages = []\n",
    "    for msg in example[\"conversations\"]:\n",
    "        if msg[\"from\"] == \"human\":\n",
    "            messages.append({\"role\": \"user\", \"content\": msg[\"value\"]})\n",
    "        elif msg[\"from\"] == \"gpt\":\n",
    "            messages.append({\"role\": \"assistant\", \"content\": msg[\"value\"]})\n",
    "\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "35d6b764dcb84e389919cafd37d324ef",
      "01e90c9462174f27abba0016d243d66b",
      "f4393720fcc540719c41fdb947bfe14a",
      "aa71d106c2cb4149941209b6b9bcd64b",
      "27abada4167d41cc8cdbd4b561acd6b4",
      "00a3940c93ad432ca732e35b098088dc",
      "94cf7261fce34d5c9d591b18e6324530",
      "18879d7f7aa24897a80ccea6431b4124",
      "2855ca8c57584a3eb00a54ab4ff7eaaf",
      "f004e1781c5247d5af33102ffc2ceb8f",
      "daaf7ad0cb514703b118c1cf7cecfbd0"
     ]
    },
    "executionInfo": {
     "elapsed": 400,
     "status": "ok",
     "timestamp": 1768437969168,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "jw4C6TfeGXZd",
    "outputId": "d9362ac0-f20f-4adc-d2cb-4360dda61afe"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d6b764dcb84e389919cafd37d324ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/305 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(tokenizer, chat_template=\"phi-3\")\n",
    "\n",
    "dataset = dataset.map(format_chat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHiVoToneynS"
   },
   "source": [
    "Let's see how the `Phi-3` format works by printing the 5th element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1768437976489,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "4GSuKSSbpYKq",
    "outputId": "d47476eb-79eb-4746-cda9-d4723fa0e5b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'from': 'human',\n",
       "  'value': 'Extract structured resume data from this CV:\\n\\nDaniel Smith daniel.smith@email.com +44 7911 123456 123 High Street, London, UK Work Experience Data Scientist at TechCorp - London March 2019 to Present Data Analyst at DataWorld - London May 2016 to February 2019 Education M.Sc. in Data Science University of London 2014 to 2016 B.Sc. in Mathematics University of Cambridge 2010 to 2014 Certifications Google Data Analytics Professional Certificate 2020 Certified Data Scientist (CDS) 2018 Projects Predictive Analytics Model Customer Segmentation Tool Financial Risk Prediction'},\n",
       " {'from': 'gpt',\n",
       "  'value': '{\"FirstName\":\"Daniel\",\"LastName\":\"Smith\",\"Address\":\"123 High Street, London, UK\",\"Phone\":\"+44 7911 123456\",\"City\":\"London\",\"Country\":\"UK\",\"Experience\":6,\"Designation\":\"Data Scientist\",\"Skill\":[\"Data Science\",\"Machine Learning\",\"Data Visualization\",\"Predictive Modeling\",\"Python\",\"R\",\"SQL\",\"Big Data\",\"Tableau\"],\"Email\":\"daniel.smith@email.com\",\"Study\":[{\"Institution\":\"University of London\",\"Degree\":\"M.Sc. in Data Science\",\"Location\":\"UK\"},{\"Institution\":\"University of Cambridge\",\"Degree\":\"B.Sc. in Mathematics\",\"Location\":\"UK\"}],\"Projects\":[{\"Project_Name\":\"Predictive Analytics Model\",\"Description\":\"Built a predictive analytics model to forecast sales trends using machine learning algorithms.\",\"Technologies\":[\"Python\",\"scikit-learn\",\"Pandas\"]},{\"Project_Name\":\"Customer Segmentation Tool\",\"Description\":\"Developed a customer segmentation tool using clustering techniques to help businesses target key demographics.\",\"Technologies\":[\"R\",\"K-means\",\"Tableau\"]},{\"Project_Name\":\"Financial Risk Prediction\",\"Description\":\"Created a financial risk prediction model based on historical data using machine learning techniques to predict potential risks.\",\"Technologies\":[\"Python\",\"TensorFlow\",\"SQL\"]}],\"Summary\":\"Experienced data scientist with a solid background in machine learning, predictive modeling, and big data technologies. Proficient in Python, R, and SQL, with a passion for solving complex problems using data-driven insights.\",\"Workplaces\":[{\"JobTitle\":\"Data Scientist\",\"Country\":\"UK\",\"City\":\"London\",\"Company_Name\":\"TechCorp\",\"Description\":\"Leading data science initiatives, building machine learning models, and deriving actionable insights from complex datasets.\",\"From_Date\":\"2019/03/01\",\"To_Date\":\"current\"},{\"JobTitle\":\"Data Analyst\",\"Country\":\"UK\",\"City\":\"London\",\"Company_Name\":\"DataWorld\",\"Description\":\"Analyzed data to identify trends, created reports, and supported decision-making processes.\",\"From_Date\":\"2016/05/01\",\"To_Date\":\"2019/02/28\"}],\"Certifications\":[{\"Institution\":\"Google\",\"Certificate Title\":\"Google Data Analytics Professional Certificate\",\"Date Of Issue\":\"2020\"},{\"Institution\":\"Certified Data Scientist (CDS)\",\"Certificate Title\":\"Certified Data Scientist\",\"Date Of Issue\":\"2018\"}]}'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5][\"conversations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1768437988132,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "U5iEWrUkevpE",
    "outputId": "e031391e-992a-4b00-f4cd-edef1a6b4ab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Extract structured resume data from this CV:\n",
      "\n",
      "Daniel Smith daniel.smith@email.com +44 7911 123456 123 High Street, London, UK Work Experience Data Scientist at TechCorp - London March 2019 to Present Data Analyst at DataWorld - London May 2016 to February 2019 Education M.Sc. in Data Science University of London 2014 to 2016 B.Sc. in Mathematics University of Cambridge 2010 to 2014 Certifications Google Data Analytics Professional Certificate 2020 Certified Data Scientist (CDS) 2018 Projects Predictive Analytics Model Customer Segmentation Tool Financial Risk Prediction<|end|>\n",
      "<|assistant|>\n",
      "{\"FirstName\":\"Daniel\",\"LastName\":\"Smith\",\"Address\":\"123 High Street, London, UK\",\"Phone\":\"+44 7911 123456\",\"City\":\"London\",\"Country\":\"UK\",\"Experience\":6,\"Designation\":\"Data Scientist\",\"Skill\":[\"Data Science\",\"Machine Learning\",\"Data Visualization\",\"Predictive Modeling\",\"Python\",\"R\",\"SQL\",\"Big Data\",\"Tableau\"],\"Email\":\"daniel.smith@email.com\",\"Study\":[{\"Institution\":\"University of London\",\"Degree\":\"M.Sc. in Data Science\",\"Location\":\"UK\"},{\"Institution\":\"University of Cambridge\",\"Degree\":\"B.Sc. in Mathematics\",\"Location\":\"UK\"}],\"Projects\":[{\"Project_Name\":\"Predictive Analytics Model\",\"Description\":\"Built a predictive analytics model to forecast sales trends using machine learning algorithms.\",\"Technologies\":[\"Python\",\"scikit-learn\",\"Pandas\"]},{\"Project_Name\":\"Customer Segmentation Tool\",\"Description\":\"Developed a customer segmentation tool using clustering techniques to help businesses target key demographics.\",\"Technologies\":[\"R\",\"K-means\",\"Tableau\"]},{\"Project_Name\":\"Financial Risk Prediction\",\"Description\":\"Created a financial risk prediction model based on historical data using machine learning techniques to predict potential risks.\",\"Technologies\":[\"Python\",\"TensorFlow\",\"SQL\"]}],\"Summary\":\"Experienced data scientist with a solid background in machine learning, predictive modeling, and big data technologies. Proficient in Python, R, and SQL, with a passion for solving complex problems using data-driven insights.\",\"Workplaces\":[{\"JobTitle\":\"Data Scientist\",\"Country\":\"UK\",\"City\":\"London\",\"Company_Name\":\"TechCorp\",\"Description\":\"Leading data science initiatives, building machine learning models, and deriving actionable insights from complex datasets.\",\"From_Date\":\"2019/03/01\",\"To_Date\":\"current\"},{\"JobTitle\":\"Data Analyst\",\"Country\":\"UK\",\"City\":\"London\",\"Company_Name\":\"DataWorld\",\"Description\":\"Analyzed data to identify trends, created reports, and supported decision-making processes.\",\"From_Date\":\"2016/05/01\",\"To_Date\":\"2019/02/28\"}],\"Certifications\":[{\"Institution\":\"Google\",\"Certificate Title\":\"Google Data Analytics Professional Certificate\",\"Date Of Issue\":\"2020\"},{\"Institution\":\"Certified Data Scientist (CDS)\",\"Certificate Title\":\"Certified Data Scientist\",\"Date Of Issue\":\"2018\"}]}<|end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[5][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1768438033946,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "YnH7gC-NHsnk",
    "outputId": "bea21196-6351-44a8-d70e-5d81fe823e54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversations': [{'from': 'human', 'value': 'Extract structured resume data from this CV:\\n\\nDaniel Smith daniel.smith@email.com +44 7911 123456 123 High Street, London, UK Work Experience Data Scientist at TechCorp - London March 2019 to Present Data Analyst at DataWorld - London May 2016 to February 2019 Education M.Sc. in Data Science University of London 2014 to 2016 B.Sc. in Mathematics University of Cambridge 2010 to 2014 Certifications Google Data Analytics Professional Certificate 2020 Certified Data Scientist (CDS) 2018 Projects Predictive Analytics Model Customer Segmentation Tool Financial Risk Prediction'}, {'from': 'gpt', 'value': '{\"FirstName\":\"Daniel\",\"LastName\":\"Smith\",\"Address\":\"123 High Street, London, UK\",\"Phone\":\"+44 7911 123456\",\"City\":\"London\",\"Country\":\"UK\",\"Experience\":6,\"Designation\":\"Data Scientist\",\"Skill\":[\"Data Science\",\"Machine Learning\",\"Data Visualization\",\"Predictive Modeling\",\"Python\",\"R\",\"SQL\",\"Big Data\",\"Tableau\"],\"Email\":\"daniel.smith@email.com\",\"Study\":[{\"Institution\":\"University of London\",\"Degree\":\"M.Sc. in Data Science\",\"Location\":\"UK\"},{\"Institution\":\"University of Cambridge\",\"Degree\":\"B.Sc. in Mathematics\",\"Location\":\"UK\"}],\"Projects\":[{\"Project_Name\":\"Predictive Analytics Model\",\"Description\":\"Built a predictive analytics model to forecast sales trends using machine learning algorithms.\",\"Technologies\":[\"Python\",\"scikit-learn\",\"Pandas\"]},{\"Project_Name\":\"Customer Segmentation Tool\",\"Description\":\"Developed a customer segmentation tool using clustering techniques to help businesses target key demographics.\",\"Technologies\":[\"R\",\"K-means\",\"Tableau\"]},{\"Project_Name\":\"Financial Risk Prediction\",\"Description\":\"Created a financial risk prediction model based on historical data using machine learning techniques to predict potential risks.\",\"Technologies\":[\"Python\",\"TensorFlow\",\"SQL\"]}],\"Summary\":\"Experienced data scientist with a solid background in machine learning, predictive modeling, and big data technologies. Proficient in Python, R, and SQL, with a passion for solving complex problems using data-driven insights.\",\"Workplaces\":[{\"JobTitle\":\"Data Scientist\",\"Country\":\"UK\",\"City\":\"London\",\"Company_Name\":\"TechCorp\",\"Description\":\"Leading data science initiatives, building machine learning models, and deriving actionable insights from complex datasets.\",\"From_Date\":\"2019/03/01\",\"To_Date\":\"current\"},{\"JobTitle\":\"Data Analyst\",\"Country\":\"UK\",\"City\":\"London\",\"Company_Name\":\"DataWorld\",\"Description\":\"Analyzed data to identify trends, created reports, and supported decision-making processes.\",\"From_Date\":\"2016/05/01\",\"To_Date\":\"2019/02/28\"}],\"Certifications\":[{\"Institution\":\"Google\",\"Certificate Title\":\"Google Data Analytics Professional Certificate\",\"Date Of Issue\":\"2020\"},{\"Institution\":\"Certified Data Scientist (CDS)\",\"Certificate Title\":\"Certified Data Scientist\",\"Date Of Issue\":\"2018\"}]}'}], 'text': '<|user|>\\nExtract structured resume data from this CV:\\n\\nDaniel Smith daniel.smith@email.com +44 7911 123456 123 High Street, London, UK Work Experience Data Scientist at TechCorp - London March 2019 to Present Data Analyst at DataWorld - London May 2016 to February 2019 Education M.Sc. in Data Science University of London 2014 to 2016 B.Sc. in Mathematics University of Cambridge 2010 to 2014 Certifications Google Data Analytics Professional Certificate 2020 Certified Data Scientist (CDS) 2018 Projects Predictive Analytics Model Customer Segmentation Tool Financial Risk Prediction<|end|>\\n<|assistant|>\\n{\"FirstName\":\"Daniel\",\"LastName\":\"Smith\",\"Address\":\"123 High Street, London, UK\",\"Phone\":\"+44 7911 123456\",\"City\":\"London\",\"Country\":\"UK\",\"Experience\":6,\"Designation\":\"Data Scientist\",\"Skill\":[\"Data Science\",\"Machine Learning\",\"Data Visualization\",\"Predictive Modeling\",\"Python\",\"R\",\"SQL\",\"Big Data\",\"Tableau\"],\"Email\":\"daniel.smith@email.com\",\"Study\":[{\"Institution\":\"University of London\",\"Degree\":\"M.Sc. in Data Science\",\"Location\":\"UK\"},{\"Institution\":\"University of Cambridge\",\"Degree\":\"B.Sc. in Mathematics\",\"Location\":\"UK\"}],\"Projects\":[{\"Project_Name\":\"Predictive Analytics Model\",\"Description\":\"Built a predictive analytics model to forecast sales trends using machine learning algorithms.\",\"Technologies\":[\"Python\",\"scikit-learn\",\"Pandas\"]},{\"Project_Name\":\"Customer Segmentation Tool\",\"Description\":\"Developed a customer segmentation tool using clustering techniques to help businesses target key demographics.\",\"Technologies\":[\"R\",\"K-means\",\"Tableau\"]},{\"Project_Name\":\"Financial Risk Prediction\",\"Description\":\"Created a financial risk prediction model based on historical data using machine learning techniques to predict potential risks.\",\"Technologies\":[\"Python\",\"TensorFlow\",\"SQL\"]}],\"Summary\":\"Experienced data scientist with a solid background in machine learning, predictive modeling, and big data technologies. Proficient in Python, R, and SQL, with a passion for solving complex problems using data-driven insights.\",\"Workplaces\":[{\"JobTitle\":\"Data Scientist\",\"Country\":\"UK\",\"City\":\"London\",\"Company_Name\":\"TechCorp\",\"Description\":\"Leading data science initiatives, building machine learning models, and deriving actionable insights from complex datasets.\",\"From_Date\":\"2019/03/01\",\"To_Date\":\"current\"},{\"JobTitle\":\"Data Analyst\",\"Country\":\"UK\",\"City\":\"London\",\"Company_Name\":\"DataWorld\",\"Description\":\"Analyzed data to identify trends, created reports, and supported decision-making processes.\",\"From_Date\":\"2016/05/01\",\"To_Date\":\"2019/02/28\"}],\"Certifications\":[{\"Institution\":\"Google\",\"Certificate Title\":\"Google Data Analytics Professional Certificate\",\"Date Of Issue\":\"2020\"},{\"Institution\":\"Certified Data Scientist (CDS)\",\"Certificate Title\":\"Certified Data Scientist\",\"Date Of Issue\":\"2018\"}]}<|end|>\\n'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuKOAUDpUeDL"
   },
   "source": [
    "If you're looking to make your own chat template, that also is possible! You must use the Jinja templating regime. We provide our own stripped down version of the `Unsloth template` which we find to be more efficient, and leverages ChatML, Zephyr and Alpaca styles.\n",
    "\n",
    "More info on chat templates on [our wiki page!](https://github.com/unslothai/unsloth/wiki#chat-templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "287326a6acde4033b9f7ffa141a439cb",
      "bc31b2de5845428b9e2cecbbe8e478a2",
      "86eebaca74d44d0a842b070c16c3c538",
      "b7b618fec7734da0ba29d4a672512f16",
      "52b1432f48334baca61fe80e9941cb47",
      "7fc2f6a7732341e98af931e5bcdfaf73",
      "98f56edb4c57400ca1eb77ada7226b23",
      "6205537a8db546a293624a477f1e1098",
      "cd7a224d93124e35b21e00b0c3566637",
      "2f854aa0b7f04d20a85d4b2d13b45d1f",
      "a868db2b4c3a4cfd9aa58da3da574867"
     ]
    },
    "executionInfo": {
     "elapsed": 1875,
     "status": "ok",
     "timestamp": 1768438199720,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "95_Nn-89DhsL",
    "outputId": "f1ae3d88-3a22-44b1-922c-47a45221f2ea"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287326a6acde4033b9f7ffa141a439cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/305 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Padding-free auto-enabled, enabling faster training.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.001,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use TrackIO/WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1768438200565,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "cfab7a2e-75a1-4132-bbb3-bfa24dcb7903"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = Tesla T4. Max memory = 14.741 GB.\n",
      "3.57 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 607827,
     "status": "ok",
     "timestamp": 1768438810474,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "8ca338c7-a082-4208-82a4-deec719e4c2d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 305 | Num Epochs = 2 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 29,884,416 of 3,850,963,968 (0.78% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 09:34, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.115100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.094600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.119100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.108300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.975500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.880700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.814900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.835100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.795800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.806500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.731800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.694400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.634800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.631000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.582600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.570400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.517400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.514400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.477300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.477200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.471700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.479500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.463300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.429400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.437600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.402400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.462900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.424400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.426700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.411100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.395800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.404800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.374200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.379500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.413600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.413700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.371900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.380400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.368300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.363700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.370500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.373900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.385300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.366700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.404900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.390700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.375700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.371200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.356600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.383600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.359100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.350200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.390600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.347900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.375400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.387800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1768438810499,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "pCqnaKmlO1U9",
    "outputId": "fbbe09f8-b19f-455d-b494-afab991d7455"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604.0377 seconds used for training.\n",
      "10.07 minutes used for training.\n",
      "Peak reserved memory = 3.57 GB.\n",
      "Peak reserved memory for training = 0.0 GB.\n",
      "Peak reserved memory % of max memory = 24.218 %.\n",
      "Peak reserved memory for training % of max memory = 0.0 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! Since we're using `Phi-3`, use `apply_chat_template` with `add_generation_prompt` set to `True` for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6012,
     "status": "ok",
     "timestamp": 1768438868149,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "kR3gIAX-SM2q",
    "outputId": "1083e1aa-dd37-427f-b541-87ba1e76d1ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|user|> Extract structured resume data from this CV:\\n\\nYuki Tanaka Tokyo yuki.tanaka@example.com +81 90 1234 5678 Work Experience Machine Learning Engineer Sony Japan - Tokyo March 2019 to Present Data Scientist Rakuten Japan - Tokyo April 2016 to February 2019 Education MSc in Artificial Intelligence University of Tokyo 2014 to 2016 BSc in Computer Science Kyoto University 2010 to 2014 Certifications AWS Certified Machine Learning Specialty 15/07/2021 Projects Autonomous Driving Algorithm Financial Forecasting System<|end|><|assistant|> {\"FirstName\":\"Yuki\",\"LastName\":\"Tanaka\",\"Address\":\"Tokyo\",\"Phone\":\"+81 90 1234 5678\",\"City\":\"Tokyo\",\"State\":\"Tokyo\",\"Country\":\"Japan\",\"Experience\":6,\"Designation\":\"']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"phi-3\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"Extract structured resume data from this CV:\\n\\n\"+ ex[\"Input\"]},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True , eos_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrSvZObor0lY"
   },
   "source": [
    " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7777,
     "status": "ok",
     "timestamp": 1768438887929,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "e2pEuRb1r2Vg",
    "outputId": "f2f81f12-970d-4af6-c53c-ff7cf0a8dd39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"FirstName\":\"Yuki\",\"LastName\":\"Tanaka\",\"Address\":\"Tokyo\",\"Phone\":\"+81 90 1234 5678\",\"City\":\"Tokyo\",\"State\":\"Tokyo\",\"Country\":\"Japan\",\"Experience\":6,\"Designation\":\"Machine Learning Engineer\",\"Skill\":[\"Machine Learning\",\"Deep Learning\",\"NLP\",\"Python\",\"TensorFlow\",\"AWS\",\"Data Science\",\"Autonomous Systems\",\"Financial Modeling\"],\"Email\":\"yuki.tanaka@example.com\",\"Study\":[{\"Institution\":\"\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"Extract structured resume data from this CV:\\n\\n\"+ ex[\"Input\"]},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128, use_cache = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 967,
     "status": "ok",
     "timestamp": 1768438891785,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "upcOlWe7A1vc",
    "outputId": "23c3cd4f-ac9b-4aee-838b-ffc0c653b729"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/chat_template.jinja',\n",
       " 'lora_model/tokenizer.model',\n",
       " 'lora_model/added_tokens.json',\n",
       " 'lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SP71TDnQOuW2"
   },
   "outputs": [],
   "source": [
    "NGROK_TOKEN = \"38FriFrvBbKt7z4l5Lz7CQBwfsE_2TwBiqkDeByf2bUbSvGSG\"\n",
    "API_KEY = \"secret123\" #password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19462,
     "status": "ok",
     "timestamp": 1768440393770,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "juWbdV8IO7bx",
    "outputId": "de09004d-4ef0-4e80-8e3e-1d74231c19ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.5.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n"
     ]
    }
   ],
   "source": [
    "pip install pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1374,
     "status": "ok",
     "timestamp": 1768440395146,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "UoyhBs-KOn2Q",
    "outputId": "e92642aa-0ed9-431d-b3ed-ac099e588a63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your public URL: https://lucio-unincarnated-leonor.ngrok-free.dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [413]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:59031 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "import uvicorn, threading, time, socket\n",
    "from pyngrok import ngrok, conf\n",
    "\n",
    "def free_port():\n",
    "    s = socket.socket()\n",
    "    s.bind(('', 0))\n",
    "    port = s.getsockname()[1]\n",
    "    s.close()\n",
    "    return port\n",
    "\n",
    "port = free_port()\n",
    "conf.get_default().auth_token = NGROK_TOKEN\n",
    "public_url = ngrok.connect(port).public_url\n",
    "print(\"Your public URL:\", public_url)\n",
    "\n",
    "def run(): uvicorn.run(app, host=\"0.0.0.0\", port=port)\n",
    "threading.Thread(target=run, daemon=True).start()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tA16pevkWThq"
   },
   "outputs": [],
   "source": [
    "mkdir AI_HR_System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1768441897326,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "YR33OVdcWVXQ",
    "outputId": "f859a0a8-f3a6-415d-8cfa-e1f1f50768a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/AI_HR_System\n"
     ]
    }
   ],
   "source": [
    "cd AI_HR_System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1768442001745,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "Jyq7kE9OWd98",
    "outputId": "98b104fa-7391-437c-ee6f-cdf7baa24f57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing api.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api.py\n",
    "import sys\n",
    "\n",
    "from fastapi import FastAPI, UploadFile, File\n",
    "import fitz\n",
    "import torch\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "def extract_text(file):\n",
    "    doc = fitz.open(stream=file, filetype=\"pdf\")\n",
    "    return \"\".join([p.get_text() for p in doc])\n",
    "\n",
    "@app.post(\"/parse\")\n",
    "async def parse_resume(file: UploadFile = File(...)):\n",
    "    text = extract_text(await file.read())\n",
    "\n",
    "    messages = [\n",
    "        {\"from\":\"human\",\"value\":\"Extract structured resume data from this CV:\\n\\n\" + text}\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=256,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "    result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return {\"result\": result}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1768442058174,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "cDymUaWFWhbr",
    "outputId": "8c1e0ca1-5055-4432-8e59-1b4fa6776275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import streamlit as st\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "BASE_URL = \"https://lucio-unincarnated-leonor.ngrok-free.dev\"\n",
    "\n",
    "PARSE_URL = BASE_URL + \"/parse\"\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"AI HR Intelligence\",\n",
    "    layout=\"wide\",\n",
    "    page_icon=\"ðŸ§ \"\n",
    ")\n",
    "\n",
    "st.title(\"ðŸ§  AI HR Intelligence System\")\n",
    "st.markdown(\"Upload a resume and let AI analyze, structure, and rank candidates.\")\n",
    "\n",
    "\n",
    "uploaded_file = st.file_uploader(\"ðŸ“„ Upload Resume (PDF)\", type=[\"pdf\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if uploaded_file and st.button(\"ðŸ” Parse Resume\"):\n",
    "    with st.spinner(\"AI is extracting candidate profile...\"):\n",
    "        try:\n",
    "            files = {\n",
    "                \"file\": (uploaded_file.name, uploaded_file, \"application/pdf\")\n",
    "            }\n",
    "            response = requests.post(PARSE_URL, files=files, timeout=120)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                parsed = response.json()[\"result\"]\n",
    "                st.session_state[\"parsed_cv\"] = parsed\n",
    "\n",
    "                st.subheader(\"ðŸ“„ Structured Candidate Profile\")\n",
    "                st.code(parsed, language=\"json\")\n",
    "\n",
    "            else:\n",
    "                st.error(\"Error parsing resume\")\n",
    "\n",
    "        except Exception as e:\n",
    "            st.error(\"Could not connect to AI server\")\n",
    "            st.write(e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1768442086338,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "Yx5EK-qdW-rF",
    "outputId": "de6ae3d9-5baf-4409-8702-d78caa2ebc94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "streamlit\n",
    "fastapi\n",
    "uvicorn\n",
    "requests\n",
    "pymupdf\n",
    "torch\n",
    "transformers\n",
    "unsloth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1768442182343,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "vp_v1tIIXMHZ",
    "outputId": "a8222aae-cc18-47a8-e3fb-ed556c5b4861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing README.md\n"
     ]
    }
   ],
   "source": [
    "%%writefile README.md\n",
    "\n",
    "# AI HR Intelligence System\n",
    "\n",
    "AI-powered resume parsing using Phi-3 + Unsloth, FastAPI, and Streamlit.\n",
    "\n",
    "Upload a PDF resume and get structured  JSON.\n",
    "\n",
    "Tech:\n",
    "- Phi-3 LLM\n",
    "- Unsloth\n",
    "- FastAPI\n",
    "- Streamlit\n",
    "- ngrok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYuevN9EXwA0"
   },
   "outputs": [],
   "source": [
    "!git config --global user.email \"mariammali258@gmail.com\"\n",
    "!git config --global user.name \"Mariaammohamed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114,
     "status": "ok",
     "timestamp": 1768443107002,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "KONVzuA6YaJZ",
    "outputId": "bc283c1b-155a-4116-bd1d-d235e0f5e5d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitialized existing Git repository in /content/AI_HR_System/.git/\n"
     ]
    }
   ],
   "source": [
    "!git init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "csLs300pYc6P"
   },
   "outputs": [],
   "source": [
    "!git add ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1768443109317,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "KpmRG0PQYews",
    "outputId": "0063b010-460b-4eb3-8986-cd134963974c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "nothing to commit, working tree clean\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"AI HR Intelligence System\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iB7ejCDxZZ8X"
   },
   "outputs": [],
   "source": [
    "!git remote remove origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6_riqMBaNnZ"
   },
   "outputs": [],
   "source": [
    "!git remote add origin https://Mariaammohamed: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@github.com/Mariaammohamed/AI-HR-Intelligence.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1112,
     "status": "ok",
     "timestamp": 1768443184953,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "4ny8wN3Rah49",
    "outputId": "9fe9163d-8d41-4101-c134-b9f67114ac1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enumerating objects: 6, done.\n",
      "Counting objects:  16% (1/6)\r",
      "Counting objects:  33% (2/6)\r",
      "Counting objects:  50% (3/6)\r",
      "Counting objects:  66% (4/6)\r",
      "Counting objects:  83% (5/6)\r",
      "Counting objects: 100% (6/6)\r",
      "Counting objects: 100% (6/6), done.\n",
      "Delta compression using up to 2 threads\n",
      "Compressing objects:  16% (1/6)\r",
      "Compressing objects:  33% (2/6)\r",
      "Compressing objects:  50% (3/6)\r",
      "Compressing objects:  66% (4/6)\r",
      "Compressing objects:  83% (5/6)\r",
      "Compressing objects: 100% (6/6)\r",
      "Compressing objects: 100% (6/6), done.\n",
      "Writing objects:  16% (1/6)\r",
      "Writing objects:  33% (2/6)\r",
      "Writing objects:  50% (3/6)\r",
      "Writing objects:  66% (4/6)\r",
      "Writing objects:  83% (5/6)\r",
      "Writing objects: 100% (6/6)\r",
      "Writing objects: 100% (6/6), 1.64 KiB | 1.64 MiB/s, done.\n",
      "Total 6 (delta 0), reused 0 (delta 0), pack-reused 0\n",
      "To https://github.com/Mariaammohamed/AI-HR-Intelligence.git\n",
      " * [new branch]      main -> main\n",
      "Branch 'main' set up to track remote branch 'main' from 'origin'.\n"
     ]
    }
   ],
   "source": [
    "!git push -u origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18883,
     "status": "ok",
     "timestamp": 1768443758956,
     "user": {
      "displayName": "Mariam Mohamed",
      "userId": "10328832704621332544"
     },
     "user_tz": -120
    },
    "id": "M005mYoFdfer",
    "outputId": "964c50a2-5e55-4ae4-b75c-695fec453abc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 572,
     "status": "ok",
     "timestamp": 1768443759519,
     "user": {
      "displayName": "Mariam Mohamed",
      "userId": "10328832704621332544"
     },
     "user_tz": -120
    },
    "id": "-03G1IZHaoAz",
    "outputId": "2a4b2179-dfb0-427e-f494-d7b4b92399e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Colab Notebooks/Phi_3.5_Mini.ipynb\n"
     ]
    }
   ],
   "source": [
    "!find /content -name \"Phi_3.5_Mini.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 136,
     "status": "ok",
     "timestamp": 1768443806354,
     "user": {
      "displayName": "Mariam Mohamed",
      "userId": "10328832704621332544"
     },
     "user_tz": -120
    },
    "id": "6XgrozqMcVqT",
    "outputId": "082dd13d-5c09-4286-ed20-9dfde52c64c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot move '/content/drive/MyDrive/Colab Notebooks/Phi_3.5_Mini.ipynb' to '/content/AI-HR-Intelligence/nb/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!mv \"/content/drive/MyDrive/Colab Notebooks/Phi_3.5_Mini.ipynb\" \"/content/AI-HR-Intelligence/nb/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 822,
     "status": "ok",
     "timestamp": 1768443909559,
     "user": {
      "displayName": "Mariam Mohamed",
      "userId": "10328832704621332544"
     },
     "user_tz": -120
    },
    "id": "NwdL6CkTeH2n",
    "outputId": "6821e5f0-366e-4bbd-d784-3a95354260c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'AI-HR-Intelligence'...\n",
      "remote: Enumerating objects: 6, done.\u001b[K\n",
      "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
      "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
      "remote: Total 6 (delta 0), reused 6 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (6/6), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Mariaammohamed/AI-HR-Intelligence.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 140,
     "status": "ok",
     "timestamp": 1768443911895,
     "user": {
      "displayName": "Mariam Mohamed",
      "userId": "10328832704621332544"
     },
     "user_tz": -120
    },
    "id": "WVX0necRdqZI",
    "outputId": "8b7ad236-9514-43e1-a70b-caa2271bb505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mAI-HR-Intelligence\u001b[0m/  \u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Syg1yx4BeAC_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "https://github.com/unslothai/notebooks/blob/main/nb/Phi_3.5_Mini-Conversational.ipynb",
     "timestamp": 1768443659653
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
